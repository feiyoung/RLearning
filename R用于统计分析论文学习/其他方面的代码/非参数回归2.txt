
setwd('C:\\Users\\Administrator\\Desktop\\新建文件夹\\非参数回归')
getwd()
rm(list=ls())
library(MASS)
library(KernSmooth)
library(locfit)
#library(help = 'KernSmooth')
#library(help ='locfit')
#save.image('nonPara_Regre.Rdata')
load('nonPara_Regre.Rdata')
Data <- read.csv('C:\\Users\\Administrator\\Desktop\\新建文件夹\\非参数回归\\nonparadata1.csv')
par(mfrow= c(1,1))
X <- Data[,2]
Y <- Data[,1]
plot(X,Y, type='p', col='blue', main='raw data plot')

#1. 核密度估计法 ------------------------------------------------------------------
#----N-W估计
#---------利用直接插入法选择最合适的窗宽
h <- dpik(X, kernel = 'box') # 选用epanech核，利用直接插入法选择核密度估计
h
est1 <- bkde(X, bandwidth = h/10, kernel = 'epanech') #返回核密度估计x,y坐标
est2 <- bkde(X, bandwidth = h, kernel = 'epanech') #核密度估计2
est3 <- bkde(X, kernel = 'epanech', gridsize = 500) # 利用核函数的标准差默认窗宽
plot(est3, type='l', xlab='x', ylab='y') # 绘出估计的和密度函数图像
#?bkde
par(mfrow=c(1,2))
plot(est1, type='l', xlab='x', ylab='y') #第一个平滑参数太小了，局部特征很明显，估计量的方差很大
plot(est2, type='l', xlab='x', ylab='y') 

#-----------不同的窗宽，相同核密度得到的核密度估计图
#窗宽越大越平滑
par(mfrow=c(2,2)) 
w=bkde(X,bandwidth=1,kernel = 'epanech');plot(w,type='l',main='h=1',xlab="",ylab="") 
w=bkde(X,bandwidth=2.6,kernel = 'epanech');plot(w,type='l',main='h=2',xlab="",ylab="") 
w=bkde(X,bandwidth=4,kernel = 'epanech');plot(w,type='l',main='h=4',xlab="",ylab="") 
w=bkde(X,bandwidth=5,kernel = 'epanech');plot(w,type='l',main='h=5',xlab="",ylab="") 

#-----------相同的窗宽，不同核密度得到的核密度估计图
par(mfrow=c(2,2)) 
w=bkde(X,kernel="epanech",bandwidth=3);plot(w,type='l',main='epanech',xlab="",ylab='')
w=bkde(X,kernel="box",bandwidth=3);plot(w,type='l',main='box',xlab="",ylab="") 
w=bkde(X,kernel="biweight",bandwidth=3);plot(w,type='l',main='biweight',xlab="",ylab='')
w=bkde(X,kernel="normal",bandwidth=3);plot(w,type='l',main='normal',xlab="",ylab='')

#-----------内核密度估计（黑线）和局部多项式估计（红线）在标准正态方法下的比较：注意两条线在两端的区别

par(mfrow=c(1,1)) 
est2<-locpoly(X,bandwidth=dpik(X))  # 局部多项式密度估计
plot(est2,type="l",xlab="X",ylab="density",col=4)  #绘出多项式密度估计图
lines(bkde(X,bandwidth=dpik(X)))  # 核密度估计图，窗宽用直接插入法，默认的高斯核
lines(locpoly(X,bandwidth=dpik(X)),col=2)  # 局部多项式密度估计，默认为高斯核
#----------N-W核回归估计回归函数
smoofit1 <- ksmooth(X, Y, "box", bandwidth = h) # h窗宽的核回归
smoofit2 <- ksmooth(X, Y, "box", bandwidth = h/10)
smoofit3 <- ksmooth(X, Y, "normal", bandwidth = h) # h窗宽的高斯核核回归
par(mfrow=c(1,1))
plot(X, Y)
lines(smoofit1, col = "red")  #绘制N-W估计图
lines(smoofit2, col = "blue") # 以更小的窗宽估计
lines(smoofit3, col = "green4") # 


#2. 局部多项式估计 -----------------------------------------------------------------
library(mgcv) # 广义加性模型包
library(help ='mgcv')
#局部多项式方法的不同窗宽下的核密度函数
h <- dpik(X, kernel = 'epanech') # 选用epanech核，利用直接插入法选择核密度估计的窗宽
h
par(mfrow=c(2,2))
est<-locpoly(X,bandwidth=40, kernel = 'epanech') 
plot(est,type="l",xlab="",ylab="density",col=3) 
est<-locpoly(X,bandwidth=90) 
plot(est,type="l",xlab="",ylab="density",col=3) 
est<-locpoly(X,bandwidth=1000) 
plot(est,type="l",xlab="",ylab="density",col=3) 
est<-locpoly(X,bandwidth=2000) 
plot(est,type="l",xlab="",ylab="density",col=3) 

#---------回归估计
h1 <- dpik(X, kernel = 'epanech') # 选用epanech核，利用直接插入法选择核密度估计
h1
par(mfrow=c(2,2))
est<-locpoly(X,Y, bandwidth=h1/10, kernel = 'epanech') #local polnomial regression
plot(X,Y,xlab="X",ylab="Y", type='p')  # visibilize
lines(est,col=2) 
plot(est,type="l",xlab="X",ylab=expression(hat(Y)),col=1) 
est<-locpoly(X,bandwidth=90) 
plot(est,type="l",xlab="",ylab="density",col=3) 
est<-locpoly(X,bandwidth=1000) # 多项式密度估计
plot(est,type="l",xlab="",ylab="density",col=3) 


#-----------------不同窗宽下局部多项式的非参数回归模型

par(mfrow=c(2,2)) 
est1<-locpoly(X,Y,degree=25,bandwidth=2) # 25次多项式
plot(X,Y,xlab="",ylab="") 
lines(est1,col=1) 
est2<-locpoly(X,Y,degree=20, bandwidth=3) 
plot(X,Y,xlab="",ylab="") 
lines(est2,col=2) 
est3<-locpoly(X,Y,degree=15, bandwidth=4) 
plot(X,Y,xlab="",ylab="") 
lines(est3,col=3) 
est4<-locpoly(X,Y,degree=10,bandwidth=5) #10次多项式效果更好
plot(X,Y,xlab="",ylab="") 
lines(est4,col=4) 

#----------相同窗宽下当局部多项式的阶数P值不同时的非参数回归模型
h2 <- dpill(X, Y) # 选用normal核，利用直接插入法选择窗宽
par(mfrow=c(2,2)) 
plot(X,Y,xlab="",ylab="") 
lines(locpoly(X,Y,drv=0L,degree=0, bandwidth=h2),col=1) #degree=0为局部常数
plot(X,Y,xlab="",ylab="") 
lines(locpoly(X,Y,drv=0L,degree=1,bandwidth=h2),col=2) # degree为局部线性
plot(X,Y,xlab="",ylab="") 
lines(locpoly(X,Y,drv=0L,degree=3,bandwidth=h2),col=3) #3次多项式
plot(X,Y,xlab="",ylab="") 
lines(locpoly(X,Y,drv=0L,degree=0, bandwidth=h2),col=1) 
lines(locpoly(X,Y,drv=0L,degree=1,bandwidth=h2),col=2) 
lines(locpoly(X,Y,drv=0L,degree=3,bandwidth=h2),col=3) 

h3 <- dpill(X,Y) # 选用normal核，利用直接插入法选择窗宽
h3
par(mfrow=c(1,2)) 
plot(X,Y,xlab="",ylab="", main='p = 0:局部常数') 
lines(locpoly(X,Y,drv=0L,degree=0, bandwidth=h3),col=1) #degree=0为局部常数
plot(X,Y,xlab="",ylab="", main='p = 1：局部线性') 
lines(locpoly(X,Y,drv=0L,degree=1,bandwidth=h3),col=2) # degree为局部线性

plot(X,Y,xlab="",ylab="", main='p = 3') 
lines(locpoly(X,Y,drv=0L,degree=3,bandwidth=h3),col=3) 
plot(X,Y,xlab="",ylab="") 
lines(locpoly(X,Y,drv=0L,degree=0, bandwidth=h3),col=1) 
lines(locpoly(X,Y,drv=0L,degree=1,bandwidth=h3),col=2) 
lines(locpoly(X,Y,drv=0L,degree=3,bandwidth=h3),col=3) 
legend("bottomright",c("p=0","p=1", 'p=3'),col=c('black','red','green'),
       lty=1, bg="white",cex=0.5)


# 选择窗宽 --------------------------------------------------------------
#install.packages('formula.tools') 处理公式的一个包，很方便
library(formula.tools)
lhs.vars(y~x) # y 返回被解释变量
rhs.vars(y~x) # x 返回解释变量名
#-------------------------------利用tricubic核函数广义交叉验证
#install.packages('locfit') #局部多项式回归包
library(locfit)
gcv.fit1 = c(0)
b = seq(from = 0.1, to = 1, by = 0.01) 
for(i in 1:length(b)) 
{ 
  fit1 = locfit(Y~lp(X, nn = b[i])) #利用默认的tricubic核        
  gcv.fit1[i] =  gcv(fit1)[4]  # 得到广义交叉验证得分
} 
gcv.fit1 = round(gcv.fit1,4) 
b1 = b[min(which.min(gcv.fit1))] 
fit1 = locfit(Y~lp(X, nn = b1))
pdf("tricubic_fit2.pdf",width=7,height=5)
plot(X,Y, type='p', col='blue', main='raw  vs estimation')
lines(fit1, pch=2, col='red')
dev.off()
pdf("tricubic_GCV2.pdf",width=7,height=5)
plot(b,gcv.fit1 ,xaxt="n", type='b',
     main='tricubic kernel', ylab='GCv score', xlab='h')
axis(side=1,at=seq(from = 0.1, to = 1.0, by = 0.1))   
dev.off()

#------------------用Epanechnic核进行广义交叉验证
gcv.fit = c(0)
b = seq(from = 0.1, to = 2, by = 0.01) 
for(i in 1:length(b)) 
{ 
  fit = locfit(Y~lp(X, nn=0.02, h = b[i]), kern='epan')  
  #fit = locfit.raw(X, Y, kern='epan', alpha=b[i])        
  gcv.fit[i] =  gcv(fit)[4] 
} 
gcv.fit = round(gcv.fit,4) 
b2 = b[min(which.min(gcv.fit))] 
fit = locfit(Y~lp(X, nn = 0.02, h=b2)) 

gcv.fit = c(0)
b = seq(from = 0.1, to = 2, by = 0.01) 
for(i in 1:length(b)) 
{ 
  fit = locfit(Y~lp(X, nn=b[i]), kern='bisq')  #原来平滑参数的大小和核函数的选择基本无关，都是0.24
  #fit = locfit.raw(X, Y, kern='epan', alpha=b[i])        
  gcv.fit[i] =  gcv(fit)[4] 
} 
gcv.fit = round(gcv.fit,4) 
(b2 = b[min(which.min(gcv.fit))] )
fit = locfit(Y~lp(X, nn = b2)) 
#----输出图像到pdf中
pdf("epan_fit2.pdf",width=7,height=5)
plot(X,Y, type='p', col='blue', main='raw  vs estimation')
lines(fit, pch=2, col='red')
dev.off()
#legend("bottomright",c('raw', 'estimation'),pch=1:2, col=c('blue','red'),bg="white",cex=0.5)
#plot(b, gcv.fit, main='Eanechnikov kernel', ylab='GCv score', xlab='h')
pdf('epan_GCV.pdf', width=7, height=5)
plot(b,gcv.fit ,xaxt="n", type='b',
     main='Eanechnikov kernel', ylab='GCv score', xlab='h')
axis(side=1,at=seq(from = 0.1, to = 2, by = 0.2))   
dev.off()
#--采用其他平滑参数值
pdf("tricubic_varous_Par.pdf",width=7,height=5)
plot(X,Y, type='p', col=4, main='raw  vs estimation')
lines(locfit(Y~lp(X, nn = 0.02, h=b2)),  col=2)
lines(locfit(Y~lp(X, nn = 0.02, h=0.1)), col=3)
lines(locfit(Y~lp(X, nn = 0.02, h=10)), col=5)
#legend("bottomright",c('h=GCV value', 'h=0.1','h=10'),
      # pch=1, col=2:4,bg="white",cex=0.5)
legend("bottomright", c('h=GCV value', 'h=0.1','h=10'), cex = 0.8, 
       lty = 1,  col = c(2:3,5), inset = 0.01, box.col = "white")
dev.off()

#----------------硕士论文非参数回归的研究及应用模拟实现
#--------模拟实验1
#数据生成
n <- 250
X <- seq(-1, 2, length=n)
truefX <- X + 5*exp(-X^2/4)
fX <- X + 5*exp(-X^2/4) + rnorm(n, 0, sd=0.1)
#1.-----N-W局部回归
estfX <- ksmooth(X, fX, kernel='normal', bandwidth = 0.4)
estfX2 <- ksmooth(X, fX, kernel='box', bandwidth = 0.4)
op <- par(mfrow=c(1,1))
plot(estfX, type='l', col='blue')
lines(X, truefX, col='red')
lines(estfX2, col='green')
points(X, fX, cex=0.1) #设置点的大小为0.1
legend('bottomright', legend = c('normal kernel', 'true curve',
                      'box kernel', 'raw data'), col=c('blue',
                      'red', 'green','black'), inset = 0.01,lty=1,
       pch=c(1,1,1, 2))
par(op)
#----局部多项式回归
estLp1 <- locpoly(X, fX, degree = 0, kernel = 'Tricubic',
                 bandwidth=0.4)  #局部常数估计
op <- par(mfrow=c(1,1))
plot(estLp1, type='l', col='blue', ylim=c(3.6, 5.5))
lines(X, truefX, col='red')
points(X, fX, cex=0.1) #设置点的大小为0.1
legend('bottomright', legend = c('tricubic kernel', 'true curve',
                                  'raw data'), col=c('blue',
                    'red', 'black'), inset = 0.01,lty=1)
par(op)
estLp2 <- locpoly(X, fX, degree = 1, kernel = 'Tricubic',
                  bandwidth=0.15)  #局部线性估计
op <- par(mfrow=c(1,1))
plot(estLp2, type='l', col='blue', ylim=c(3.6, 5.5))
lines(X, truefX, col='red')
points(X, fX, cex=0.1) #设置点的大小为0.1
legend('bottomright', legend = c('tricubic kernel', 'true curve',
                                 'raw data'), col=c('blue',
                                                    'red', 'black'), inset = 0.01,lty=1)
par(op)
#--------------模拟实验2
#数据生成
n <- 1000
X <- seq(0, 1, length=n)
truefX <- sqrt(X*(1-X))*sin(2.1*pi/(X+0.05)) 
fX <- truefX + rnorm(n, 0, sd=0.1)
#--非参数局部线性回归分析方法
estLp2 <- locpoly(X, fX, degree = 1, kernel = 'Tricubic',
                  bandwidth=0.001)  #局部线性估计
op <- par(mfrow=c(1,1))
plot(X, fX, cex=0.1)
lines(estLp2,col='blue')
lines(X, truefX, col='red')
legend('bottomright', legend = c('tricubic kernel', 'true curve',
                                 'raw data'), col=c('blue',
                                                    'red', 'black'), inset = 0.01,lty=1)
par(op)
#------------------用tricubic核进行广义交叉验证
gcv.fit = c(0)
b = seq(from = 0.001, to = 1, by = 0.001) 
for(i in 1:length(b)) 
{ 
  fit = locfit(fX~lp(X, nn=0.02, h = b[i]), kern='tricubic')  
  #fit = locfit.raw(X, Y, kern='epan', alpha=b[i])        
  gcv.fit[i] =  gcv(fit)[4] 
} 
gcv.fit = round(gcv.fit,4) 
b2 = b[min(which.min(gcv.fit))] 
fit = locfit(fX~lp(X, nn = 0.02, h=b2)) 
lines(fit, col='green')
#--------------------------
save.image('nonPara_Regre.Rdata')

#----------------------何叶师姐的代码
rm(list=ls()) #清除环境变量
par(mfrow=c(1,2)) # 设置绘图布局
data=read.csv("nonparadata1.csv") # 载入数据
y=data[,1] 
x=data[,2]

##选择窗宽h
N <- 20
CV0<-outer(1:N,1:2)*0  # 初始化6*2的一个矩阵 （1:6）6*1 *(1:2)1*2 = 6*2
CV0[,1]<-seq(0.3, 5, length=N)#c(0.6,0.8,1,1.2,1.5,1.8) # 给第一列赋值，给定一系列的窗宽参数
j<-1
while(j<=N) # 循环
{
  #j <- 1
  h<-CV0[j,1] # 窗宽参数取便第一列
  RE=0 # 初始化
  for (i in 1:5) # 5折交叉验证，共300个观测
  {
    #i <- 5
    index=1:60+(i-1)*60  #index=rep(1,60)+(i-1)*60 ;index=(1:60)+(i-1)*60
    x0=x[index] # 取出剔除的数据
    y0=y[index]
    yy=y[-index] # 剔除这一组数据
    xx=x[-index]
    sum=0 # 初始化
    for(k in 1:60) #缺一交叉验证
    {
     # k <- 45
      #kernel=0.75*(1-((xx-x0[k])/h)^2)/h*(abs(xx-x0[k])<=h) # epanchnnekov核函数
      kernel <- 1/sqrt(2*pi)*exp(-(xx-x0[k])^2/h^2)/h
      x1 <- xx-x0[k]
      f=lm(yy~x1, weights=kernel) # 带权的线性回归
      sum=sum+(y0[k]-f$coefficient[1])^2 # 计算误差平方和
    }
    RE=RE+sum # 得到误差平方和
  }
  CV0[j,2]=RE # 第二列为对应误差平方和
  print(CV0[j,2]) #输出误差平方和
  j<-j+1
}
#dput(CV0,"CV0_h.r") # 将R对象CVO写入到一个二进制文本文件中
op <- par(mfrow=c(1,1))
plot(CV0[,1],CV0[,2],xlab="h",ylab="LOOCV",main="(a) h vs cv") # 会出图像
lines(CV0[,1],CV0[,2])
(h <- CV0[which.min(CV0[,2]),1]) #
#图形
y=data[,1]
x=data[,2]
plot(x,y,main="(b) Estiamted curve")
n=length(x)
x0=unique(sort(x)) # 对x按升序排列
n0=length(x0)
#h=1 # 利用K折交叉验证选择的窗宽
result=matrix(0,n0,2) # n0*2的矩阵
for (i in 1:n0)
{
  Y=y
  X=x-x0[i]
  
  #kernel=0.75*(1-((x-x0[i])/h)^2)/h*(abs(x-x0[i])<=h)
  kernel=exp(-(x-x0[i])^2/(2*h^2))/(h*sqrt(2*pi)) # guassian kernel, don't occur all-zero wieght matrix
  f=lm(Y~X,weights=kernel)
  result[i,1]=f$coefficient[1] # 局部线性估计整条曲线
  result[i,2]=x0[i] # 为对应x的取值
}
lines(result[,2],result[,1], col='red')
#write.table(result,"estimate.r") # 将结果变量导入到文件中

#---------------重抽样计算标准差并画出置信带

n1=200 # 重抽样200次
re=matrix(0,n1,n0)  # 200*n0的矩阵

for(i in 1:n1)
{
  set.seed(i)
  index<-sample(1:n,n,T,rep(1/n,n))
  y=data[index,1] #只是将x和y的顺序打乱
  x=data[index,2]
  for (j in 1:n0)
  {
    Y=y
    X=x-x0[j]
    kernel=0.75*(1-((x-x0[j])/h)^2)/h*(abs(x-x0[j])<=h)
    #kernel=exp(-(x-x0[j])^2/(2*h^2))/(h*sqrt(2*pi))
    f=lm(Y~X,weights=kernel)
    re[i,j]=f$coefficient[1]
  }
  print(i)
}
#write.table(re,"residual.r")
#result=read.table("estimate.r")
#re=read.table("residual.r")
sd=rep(0,n0) #初始化标准差矩阵
for (k in 1:n0)
{
  sd[k]=sd(re[,k]) # 计算每一列的标准差
}

lines(result[,2],result[,1]-1.96*sd,lty=2, col='blue')
lines(result[,2],result[,1]+1.96*sd,lty=2, col='blue')





